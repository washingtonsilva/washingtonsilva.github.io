[
  {
    "objectID": "publications/vies_confirmacao.html",
    "href": "publications/vies_confirmacao.html",
    "title": "Viés de confirmação na tomada de decisão gerencial: um estudo experimental com gestores e contadores",
    "section": "",
    "text": "Abstract\nThis study aimed to analyze the presence of confirmation bias in managers and accountants in a management decision-making process, as well as to analyze whether the way in which economic-financial information is presented influences the confirmation bias of these individuals in their decisions. To meet the research objective, we used the experimental methodology, applied to a sample of 86 Accountants, 68 managers and 118 people with various activities (control group). The results showed that most managers and accountants have confirmation bias in management decision-making processes, that type (positive or negative) and level (simple or complex) factors of information do not influence the confirmation bias in the management decision-making and that negative information may influence the confirmation bias in choosing the most important information."
  },
  {
    "objectID": "publications/time_speckle.html",
    "href": "publications/time_speckle.html",
    "title": "Time history speckle pattern under statistical view",
    "section": "",
    "text": "Abstract\nThe dynamic speckle analysis has been done because speckle interference began to be evaluated as an important source of information, especially those related to biological samples under laser beam. The time history speckle pattern, a THSP image, is an approach to analyze, which allows a summary to represent the activity monitored. THSP adoption has compelled the research to evaluate the information and the reliability of its inner pattern. This study presents a statistical approach to analyze the data using white noise tests and cross-spectrum analysis. The data analyzed was a set of THSP from animal sperm samples. The evaluation of white noise among lines was conducted using the Fischer test over eight THSP and the cross-spectral approach has been conducted by comparing the information in each THSP. The results presented that the THSP of the sperm did not behave as a white noise in a global evaluation, and specifically in the Gaussian white noise test over 4096 lines; only six lines presented the behavior of a white noise pattern. The strong rejection of white noise test confirms that the THSP pattern allows reliable information. The results obtained with the cross-spectral analysis presented differences between lines within the same THSP, showing that the information of an inner pattern varies in relation to space, which is against the findings in literature."
  },
  {
    "objectID": "publications/premio_voto.html",
    "href": "publications/premio_voto.html",
    "title": "Determinantes pelo Prêmio pelo Direito de Voto no Mercado de Ações Brasileiro",
    "section": "",
    "text": "Abstract\nThis study aims identify the determinants of premium for voting rights in the Brazilian market. To this end, we developed a dataset consisted of 383 companies listed on the Brasil, Bolsa, Balcão (B3) from 2000 to 2015, to use the fixed effects regressions with panel data models. We identified that the average of the premium found for the period is 18.07%, however, this value is close to 40.00% in years considered of systemic crisis, as occurred in 2008 and 2015. In addition to the positive influence of the years of decline in the stock market in the value of the premium, the results showed that i) the increase in liquidity of shares with voting rights negatively affect the value of the premium for control; and ii) the level of liabilities payable in relation to assets has an inverted U-shape in relation to the voting premium. The study contributes to the literature by verifying that a structure of two classes of shares allows the accentuation of the premium for voting rights in periods of systematic crisis."
  },
  {
    "objectID": "publications/natan01.html",
    "href": "publications/natan01.html",
    "title": "Portfolio optimization based on the pre-selection of stocks by the Support Vector Machine model",
    "section": "",
    "text": "Abstract\nThis study aims to analyze the performance of an investment portfolio using the Markowitz model, which maximizes the Sharpe ratio from a set of assets preselected through the Support Vector Machine (SVM) model using fundamental indicators in the Brazilian stock market. With an accuracy of 61% for the SVM model, the results indicate that preselecting assets based on fundamental indicators and subsequently optimizing them by maximizing the Sharpe ratio showed a superior return and faster recovery after drawdown periods compared to the benchmark or SVM (1/n) strategy. These results suggest the relevance of including the SVM in the optimization portfolio process."
  },
  {
    "objectID": "publications/heavy_metals.html",
    "href": "publications/heavy_metals.html",
    "title": "Can moderate heavy metal soil contaminations due to cement production influence the surrounding soil bacterial communities?",
    "section": "",
    "text": "Abstract\nEvents of soil contamination by heavy metals are mostly related to human activities that release these metals into the environment as emissions or effluents. Among the industrial activities related to heavy metal pollution, cement production plants are considered one of the most common sources. In this work we applied the High-throughput sequencing approach called 16 S rDNA metabarcoding to perform the taxonomic characterization of the prokaryotic communities of the soil surrounding three cement plants as well as two areas outside the influence of the cement plants that represented agricultural production environments free of heavy metal contamination (control areas). We applied the environmental genomics approaches known as “structural community metrics” (α- and β-diversity metrics) and “functional community metrics” (PICRUSt2 approach) to verify whether or not the effects of heavy metal contamination in the study area generated impacts on soil bacterial communities. We found that the impact related to the elevation of heavy metal concentration due to the operation of cement plants in the surrounding soil can be considered smooth according to globally recognized indices such as Igeo. However, we identified that both the taxonomic and functional structures of the communities surrounding cement plants were different from those found in the control areas. We consider that our findings contribute significantly to the general understanding of the effects of heavy metals on the soil ecosystem by showing that light contamination can disturb the dynamics of ecosystem services provided by soil, specifically those associated with microbial metabolism."
  },
  {
    "objectID": "publications/estrutura_capital.html",
    "href": "publications/estrutura_capital.html",
    "title": "Capital Structure and Information Asymmetry: A Study of Brazilian Publicly Traded Companies of Testile and Electricity Industries",
    "section": "",
    "text": "Abstract\nGiven the various theories of capital structure and the Pecking Order theory, the present study related the information asymmetry and the capital structure of companies aiming to verify if information asymmetry affects the definition of the capital structure of Brazilian companies held in the electric power industry and textile. The research involved a sample of 53 companies, 31 of the electricity and 22 textiles sector during the years 2008 to 2012. The methodology used was regression with panel data, it allows the same unit cross-sectional monitored over time. The results showed that information asymmetry is an important determinant of capital structure, but that the sectors diverge with respect to the Pecking Order Theory. In the case of the energy sector to reduce information asymmetry led to a propensity for debt, while the textile sector, the opposite occurred, as companies less susceptible to information asymmetry are averse to debt."
  },
  {
    "objectID": "publications/biospecke.html",
    "href": "publications/biospecke.html",
    "title": "Biospeckle laser spectral analysis under Inertia Moment, Entropy and Cross-Spectrum methods",
    "section": "",
    "text": "Abstract\nBiospeckle or dynamic speckle can be used as a method for analysing activity, biologic or not, from materials illuminated with laser beam. The Spatial Temporal Speckle (STS) contains data of time information of dynamic speckle and it is used as input for many techniques allowing the analysis of the activity which is being monitored. One question that rises from the manipulation of the STS is related with the information inside it, in particular, whether it is possible to access different frequency behaviors in the time series presented in the STS pattern. This study presents the Inertia Moment, the Wavelets based Entropy and the Cross-Spectrum analysis as approaches that can be used for evaluating the STS spectral content. In a simulation, STS lines have been created based on many frequencies of the fundamental harmonic. This was done for verifying as each method acts when analysing different frequencies, varying harmonics offset and amplitude. These techniques were applied to real database, to validate their action mechanism in real samples. The results present that all techniques were able to verify the spectral content of different harmonics. Inertia Moment was more efficient on analysing high frequencies, because it is a second order moment, being able to obtain more information from high variations on activity. Entropy and Cross-Spectrum, in turn, were better on differing lower frequencies. This was attributed to the convolution proccess, which is present in both methods, filtering high frequencies. Although, any of them returned informations on both high and low frequencies at the same time, they can be used simultaneously, since Entropy and Cross-Spectrum were complementary to Inertia Moment."
  },
  {
    "objectID": "publications/anchoring_effect.html",
    "href": "publications/anchoring_effect.html",
    "title": "Anchoring effect in managerial decision-making in accountants and managers: an experimental study",
    "section": "",
    "text": "Abstract\nThe objective of this work was to analyze, by means of an experiment, if the type (positive or negative) and the level (simple or complex) of economic-financial information influence the anchoring effect of accountants and managers in a process of managerial decision-making. To do so, an experimental methodology targeting a sample of 86 Accountants, 68 Managers and 118 people with different professional activities (control group) was used. The results showed, in the first test without differentiation of factors (type and level), that about 96% of the participants have the anchoring effect, leaning towards minimum and maximum estimates of sales revenue, operating expenditure and result. In addition, the ANOVA and the Approximate Permutation Test brought significant evidence that the anchoring effect in minimum projections can be influenced by the type of information, not being significant for anchoring in maximum projections and for the level of information on both estimates (minimum and maximum). Finally, the conclusion is that positive information increases the anchoring effect and negative information decreases the anchoring effect in minimum estimates in relation to the low anchor."
  },
  {
    "objectID": "tutorials/index.html",
    "href": "tutorials/index.html",
    "title": "Tutoriais",
    "section": "",
    "text": "Esta página reúne tutoriais e materiais didáticos sobre Estatística, Econometria, Ciência de Dados, R, Git, Python e outros recursos.\n\n\n\n\n\n\n\n\n\n\n\n\nControle de Versão com Git e GitHub\n\n\n\n\n\n\ncontrole de versão\n\n\ngit\n\n\ngitHub\n\n\n\nUm tutorial básico sobre o uso do sistema de controle de versão Git e sobre o GitHub \n\n\n\n\n\n7 outubro 2024\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "tutorials/dbca/index.html",
    "href": "tutorials/dbca/index.html",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "",
    "text": "Os conceitos a seguir foram baseados em Casella (2008).\n\n\n\n\n\n\nUnidade Experimental\n\n\n\nA unidade experimental é a unidade (sujeito, planta, vaso, animal…) que é atribuído aleatoriamente a um tratamento.\n\n\nA unidade experimental, como o nome indica, é a unidade básica do experimento, e define a unidade a ser replicada para aumentar os graus de liberdade. Na definição de unidade experimental, a frase “atribuída aleatoriamente” é de importância crucial.\n\n\n\n\n\n\nUnidade de Amostragem\n\n\n\nUma unidade de amostragem é o objeto que é medido em um experimento. Pode ser diferente da unidade experimental.\n\n\n\n\n\n\n\n\nRepetição\n\n\n\nO princípio da repetição consiste em aplicarm o mesmo tratamento a várias unidades experimentais em um mesmo experimento.\n\n\nO princípo da repetição permite obter a variação entre unidades experimentais, ou seja, permite obter uma estimativa do erro experimental.\n\n\n\n\n\n\nAleatorização\n\n\n\nTalvez o princípio mais fundamental da experimentação seja a aleatorização, ou seja, obter as observações (ou, mais precisamente, as unidades experimentais) em um forma aleatória que é tão livre de viés quanto possível.\n\n\nA ideia básica da aleatorização é que, dado um delineamento, a atribuição das unidades experimentais aos tratamentos deve ser escolhida aleatoriamente, com igual probabilidade, entre todas as atribuições possíveis. Essa estratégia resulta em uma amostra aleatória simples.\nA atribuição aleatória de unidades experimentais a tratamentos deve resultar nos seguintes resultados desejáveis:\nEliminação de viés sistemático. O viés vem em muitas formas, e algumas delas são desconhecidas. Alguns exemplos são gradientes de luz ou temperatura, tendência de entrevistador em pesquisas e outras ocorrências. A aleatorização é uma maneira de quebrar um efeito sistemático.\nObtenção de uma amostra representativa. O objetivo final em qualquer experimento é fazer uma inferência válida para uma população, portanto, os dados devem ser representativos dessa população. A aleatorização é necessária para se obter uma amostra representativa.\nControla variáveis ou fatores de confundimento\nFatores de confundimento existem em todas os experimentos. Por exemplo, considere um experimento com seres humanosp para testar o efeito de dietas na pressão arterial, medida em 12 indivíduos. Embora possamos recrutar sujeitos com um estado geral de saúde semelhante, fatores de confundimento como estilo de vida, raça, disposição genética ou muitos outros fatores podem influenciar os resultados.\nApesar de ser possível controlar alguns fatores de confundmento, tal como raça, outros fatores são incontroláveis, e até mesmo desconhecidos, como a disposição genética. A aleatorização ajudará a distribuir essa variação desconhecida ao longo do experimento, quebrando a influência do fator de confundimento."
  },
  {
    "objectID": "tutorials/dbca/index.html#conceitos-básicos",
    "href": "tutorials/dbca/index.html#conceitos-básicos",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "",
    "text": "Os conceitos a seguir foram baseados em Casella (2008).\n\n\n\n\n\n\nUnidade Experimental\n\n\n\nA unidade experimental é a unidade (sujeito, planta, vaso, animal…) que é atribuído aleatoriamente a um tratamento.\n\n\nA unidade experimental, como o nome indica, é a unidade básica do experimento, e define a unidade a ser replicada para aumentar os graus de liberdade. Na definição de unidade experimental, a frase “atribuída aleatoriamente” é de importância crucial.\n\n\n\n\n\n\nUnidade de Amostragem\n\n\n\nUma unidade de amostragem é o objeto que é medido em um experimento. Pode ser diferente da unidade experimental.\n\n\n\n\n\n\n\n\nRepetição\n\n\n\nO princípio da repetição consiste em aplicarm o mesmo tratamento a várias unidades experimentais em um mesmo experimento.\n\n\nO princípo da repetição permite obter a variação entre unidades experimentais, ou seja, permite obter uma estimativa do erro experimental.\n\n\n\n\n\n\nAleatorização\n\n\n\nTalvez o princípio mais fundamental da experimentação seja a aleatorização, ou seja, obter as observações (ou, mais precisamente, as unidades experimentais) em um forma aleatória que é tão livre de viés quanto possível.\n\n\nA ideia básica da aleatorização é que, dado um delineamento, a atribuição das unidades experimentais aos tratamentos deve ser escolhida aleatoriamente, com igual probabilidade, entre todas as atribuições possíveis. Essa estratégia resulta em uma amostra aleatória simples.\nA atribuição aleatória de unidades experimentais a tratamentos deve resultar nos seguintes resultados desejáveis:\nEliminação de viés sistemático. O viés vem em muitas formas, e algumas delas são desconhecidas. Alguns exemplos são gradientes de luz ou temperatura, tendência de entrevistador em pesquisas e outras ocorrências. A aleatorização é uma maneira de quebrar um efeito sistemático.\nObtenção de uma amostra representativa. O objetivo final em qualquer experimento é fazer uma inferência válida para uma população, portanto, os dados devem ser representativos dessa população. A aleatorização é necessária para se obter uma amostra representativa.\nControla variáveis ou fatores de confundimento\nFatores de confundimento existem em todas os experimentos. Por exemplo, considere um experimento com seres humanosp para testar o efeito de dietas na pressão arterial, medida em 12 indivíduos. Embora possamos recrutar sujeitos com um estado geral de saúde semelhante, fatores de confundimento como estilo de vida, raça, disposição genética ou muitos outros fatores podem influenciar os resultados.\nApesar de ser possível controlar alguns fatores de confundmento, tal como raça, outros fatores são incontroláveis, e até mesmo desconhecidos, como a disposição genética. A aleatorização ajudará a distribuir essa variação desconhecida ao longo do experimento, quebrando a influência do fator de confundimento."
  },
  {
    "objectID": "tutorials/dbca/index.html#visão-geral",
    "href": "tutorials/dbca/index.html#visão-geral",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "Visão Geral",
    "text": "Visão Geral\nEm muitas situações sabemos que as unidades experimentais não são homogêneas, e fazer o uso explícito da estrutura especial das unidades experimentais geralmente ajuda a reduzir o erro experimental (Meier (2022)).\nEm um curso de estatística básica, aprendemos como aplicar o teste t emparelhado. Esse teste é utilizado em situações em que dois tratamentos são aplicados no mesmo “objeto” ou “sujeito”. Pense, por exemplo, na aplicação de dois tratamentos, em paralelo, em seres humanos, como a aplicação de dois tipos de colírios diferentes, cada um aplicado em um dos dois olhos.\nSabemos que os indivíduos podem ser muito diferentes, mas devido ao fato de aplicarmos ambos os tratamentos no mesmo indivíduo, obtemos uma imagem clara do efeito do tratamento em cada indivíduo, tomando a diferença dos valores da variável resposta correspondentes aos dois tratamentos.\nIsso faz com que a variabilidade de sujeito para sujeito seja muito reduzida. Também dizemos que bloqueamos os sujeitos ou que um sujeito individual é um bloco.\nSob uma abordagem mais geral, conforme Montgomery (2001), podemos afirmar que em qualquer experimento, a variabilidade proveniente de um fator de perturbação pode afetar os resultados.\nEm geral, definimos um fator de perturbação como um fator do delineamento que, provavelmente tem um efeito sobre a variável resposta (y), mas em cujo efeito o experimentador não está interessado. Assim, a variabilidade que ele pode transmitir para a variável resposta deve ser minimizada.\nEm alguns experimentos, o fator de perturbação é desconhecido e incontrolável, isto é, não sabemos se o fator está afetando a variável resposta e se os seus níveis são os mesmos durante a realização do experimento. O princípio da experimentação que denominamos aleatorização é a técnica que permite ao experimentador proteger o experimento desse fator de perturbação à espreita.\nEm outros casos, o fator de perturbação é conhecido, mensurável, mas incontrolável. Se o experimentador conseguir pelo menos observar os valores desse fator em cada execução do experimento, ele pode ser incorporado na análise pela técnica denominada Análise de Covariância\nQuando o fator de perturbação da variabilidade é conhecido e controlável, o experimentador utiliza a técnica denominada blocagem para minimizar ou reduzir sistematicamente seu efeito sobre as comparações das médias dos tratamentos. A blocagem é uma técnica de experimentação extremamente importante e usada extensivamente.\nFatores de perturbação, conhecidos e controláveis, são chamados de blocos. O objetivo da blocagem é tornar um ambiente heterogêneo em subambientes homogêneos.\nFatores de perturbação (blocos) típicos incluem localização, o tempo, se um experimento é realizado em diferentes períodos de tempo (dia, semestre, ano etc.), pessoas, etc.\nEm inglês, um delineamento em blocos completamente aleatorizados (DBCA) pode ser traduzidp por Randomized Complete Block Design (RCBD). O que significa cada termo?\n\nRandomized: os tratamentos sáo atribuídos aleatoriamente dentro de cada bloco.\nComplete: Todos os tratamentos estão presentes em todos os bloco e cada tratamento é utilizado o mesmo número de vezes, geralmente uma vez, dentro de cada bloco.\nBlock: as unidades experimentais são agrupadas de forma a criar subgrupos homogênos.\nDesign: É o seu experimento.\n\n\n\n\n\n\n\nExemplo\n\n\n\nUm estudo no qual os participantes são, inicialmente, divididos em blocos (subconjuntos relativamente homogêneos) de acordo com alguma característica (por exemplo, idade) que não é um foco de interesse e são então atribuídos, aleatoriamente, aos diferentes tratamentos de tal forma que cada tratamento aparece uma vez em cada bloco.\nAssim, o número de participantes em cada bloco deve ser igual ao número de níveis do tratamento. Por exemplo, o seguinte arranjo de um tratamento com quatro níveis (A, B, C, D) e 16 indivíduos (de quatro grupos etários) é um delineamento em blocos completos:\n\n\n\n\nTratamentos\n\n\n\n\n\n\n\nBloco\n\n\n\n\n\n\ncriancas\nA\nB\nC\nD\n\n\nadolescentes\nB\nC\nD\nA\n\n\nadultos\nC\nD\nA\nB\n\n\nidosos\nD\nA\nB\nC\n\n\n\nAo garantir que o fator de perturbação (aqui, idade) seja igualmente representado em todos as condições experimentais, um DBC reduz ou elimina sua contribuição para o erro experimental."
  },
  {
    "objectID": "tutorials/dbca/index.html#dbca-modelo-linear-normal---efeitos-fixos.",
    "href": "tutorials/dbca/index.html#dbca-modelo-linear-normal---efeitos-fixos.",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "DBCA: Modelo Linear Normal - Efeitos Fixos.",
    "text": "DBCA: Modelo Linear Normal - Efeitos Fixos.\nO modelo estatístico linear associado a um DBCA pode ser representado por,\n\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij}, \\, \\text{sendo}\\, \\epsilon_{ij} \\sim i.i.d \\quad N(0,\\sigma^2)\n    \\begin{cases}\n    i = 1,\\ldots,a \\\\\n    j =  1,\\ldots,b\n    \\end{cases}\n Sendo:\n\\mu = média global de y.\n\\tau_i = efeito em y_{i.} devido ao tratamento i. Ou seja, o efeito do tratamento i. Sáo também denominados como efeitos principais.\n\\beta_j = efeito (sem interesse) do bloco j.\ny_{ij} = resposta observada na unidade j devida ao tratamento i.\n\\epsilon_{ij} = erro não observado (ou erro experimental).\nO DBCA é um modelo linear, portanto, assume que entre os blocos e os níveis dos fatores."
  },
  {
    "objectID": "tutorials/dbca/index.html#dbca-tabela-da-análise-da-variância---efeitos-fixos",
    "href": "tutorials/dbca/index.html#dbca-tabela-da-análise-da-variância---efeitos-fixos",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "DBCA: Tabela da Análise da Variância - Efeitos Fixos",
    "text": "DBCA: Tabela da Análise da Variância - Efeitos Fixos\n\\begin{array}{l|l|l|l|l}\n        \\hline\n\\text{Fonte de Variação} & \\text{gl}  & \\text{SQ}        & \\text{QM}                       & F_{calculado} \\\\\n        \\hline\n        \\text{tratamentos}  & a-1        & SQ_{tratamentos}  & \\frac{SQ_{tratamentos}}{a-1}    & \\frac{QM_{tratamentos}}{QM_{resíduos}}    \\\\\n        \\text{blocos}       & b-1        & SQ_{blocos}       & \\frac{SQ_{blocos}}{b-1}         &                 \\\\\n        \\text{resíduo}      & (a-1)(b-1) & SQ_{resíduo}      & \\frac{SQ_{resíduo}}{(a-1)(b-1)} &                 \\\\\n        \\hline\n        \\text{total}        & ab - 1    & SQ_{total}         &                                 &                 \\\\\n        \\hline\n\\end{array}\nF_{tabelado}=F_{\\nu_1 = (a-1), \\nu_2 = (a-1)(b-1)}.\n\n\n\n\n\n\nEstimador do Erro Experimental\n\n\n\nO quadrado médio do resíduo:\n\nQM_{resíduo} = \\frac{SQ_{resíduo}}{(a-1)(b-1)}\n é o estimador do erro experimental no contexto deste modelo de um DBCA.\n\n\nConsiderando que são a níveis do fator de interesse e b blocos. A hipótese de interesse é:\n\n\\begin{cases}\nH_o:  & \\mu_1 = \\mu_2 = \\ldots = \\mu_a \\,\\, \\text{(as médias dos tratamentos são iguais)}  \\\\\nH_a:  & \\text{Pelo menos duas médias são diferentes}\n\\end{cases}\n\nRegras de Decisão do teste F:\nSe F_{calculado} &gt; F_{tabelado}, rejeitamos a hipótese nula de que as médias dos tratamentos são iguais, caso contrátio, náo rejeitamos a hipótese nula, dado o nível de significância \\alpha escolhido.\nEntretanto, a alternativa mais comum para interpretar o resultado do teste F é utilizar o valor-p do teste. Por exemplo, caso adote-se \\alpha = 0.05 = 5%, a regra de decisão é:\n\nSe valor-p &lt; 0.05, o teste fornece evidência para rejeitar a hipótese nula de que as médias dos tratamentos são iguais.\nSe valor-p &gt; 0.05, o teste fornece evidência para rejeitar a hipótese nula de que as médias dos tratamentos são iguais."
  },
  {
    "objectID": "tutorials/dbca/index.html#sumário-das-principais-características-de-um-dbc",
    "href": "tutorials/dbca/index.html#sumário-das-principais-características-de-um-dbc",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "Sumário das Principais Características de um DBC",
    "text": "Sumário das Principais Características de um DBC\nUso:\n\nApropriado quando se tem um fator de perturbação atuando sobre a variável resposta.\nO ideal é que cada tratamento ocorra em cada bloco o mesmo número de vezes, usualmente uma vez.\nDeve-se minimizar a variação dentro de cada bloco e maximizar a variação entre os blocos.\n\nVantagens:\n\nControla um fator de perturbação (conhecido e controlável) que pode atuar sobre a variável resposta (tempo, temperatura etc.), aumentando a precisão do experimento, ou seja, reduzindo o erro experimental.\nUm DBCA pode acomodar qualquer número de tratamentos e qualquer número de blocos. Entretanto, o ideal é que cada tratamento seja replicado o mesmo número de vezes em cada bloco.\nUtilizando blocos com diferentes condições, os resultados do experimento podem ter uma amplitude maior.\nA análise estatística é simples.\n\nDesvantagens:\n\nDados faltantes reduzem o poder dos testes e a cobertura dos intervalos de confiança.\nSe existir mais de uma fator de perturbação, conhecido e controlável, a utilização do DBCA é ineficiente.\nSe as condições experimentais forem de fato homogêneas, um delineamento inteiramente aleatorizado é mais eficiente que o DBCA.\nNa medida em que aumenta-se o número de tratamentos, mais itens heterogêneos podem ser utilizados e a blocagem correta torna-se cada vez mais complexa, neste caso outros delineamentos podem ser mais eficientes."
  },
  {
    "objectID": "tutorials/dbca/index.html#etapas-da-análise-de-um-dbca",
    "href": "tutorials/dbca/index.html#etapas-da-análise-de-um-dbca",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "Etapas da Análise de um DBCA",
    "text": "Etapas da Análise de um DBCA\n\nEstruturação e Importação do Arquivo de Dados.\nAnálise Exploratória dos Dados\nEstimação do Modelo Linear Normal da ANOVA.\nDiagnóstico do Modelo (verificar as hipóteses do modelo linear normal).\nComparações Múltiplas.\n\nAs etapas típicas da análise dos dados produzidos por um DBCA serão ilustradas pela resolução das questões (1 a 9) associadas ao seguinte experimento:"
  },
  {
    "objectID": "tutorials/dbca/index.html#análise",
    "href": "tutorials/dbca/index.html#análise",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "Análise",
    "text": "Análise\nVamos ilustrar a análise de dados de um experimento que utilizou um DBCA utilizando o seguinte experimento:\n\n\n\n\n\n\nExperimento\n\n\n\nTrês soluções de lavagem estão sendo comparadas para avaliar a sua eficiência em retardar o crescimento de bactérias em containers de leite. A Análise é feita em um laboratório, e apenas três ensaios podem ser executados em qualquer dia. O experimentador decide usar um delineamento em blocos completos aleatorizados (DBCA). As observações são tomadas durante quatro dias, e os dados são apresentados na tabela abaixo.\n\n\n\n1. Identifique o bloco (fator de perturbação), o fator de interesse (tratamento) e seus níveis e a variável resposta deste experimento. Além disso, descreva o modelo linear do experimento.\n\nFator de Perturbação/Blocos: dias.\nFator de Interesse/Tratamento: soluções para lavagem de containers de leite.\nNíveis do Fator de Interesse/Tratamento: As três soluções de lavagem distintas\nvariável Resposta (y)]: O crescimento, medido pela contagem, de bactérias\n\nSendo o crescimento (contagem) de bactérias a variável resposta e representada por y_{ij}, as solucões testadas os tratamentos e representados por sol, os quatro dias os blocos e representados por dia, podemos representar o modelo linear do experimento como:\n\ny_{ij} = \\mu +\n                      \\tau_1 sol_{1} +\n                      \\tau_2 sol_{2} +\n                      \\tau_3 sol_{3} +\n                      \\beta_{1}dia_{1} +\n                      \\beta_{2}dia_{2} +\n                      \\beta_{3}dia_{3} +\n                      \\beta_{4}dia_{4} +\n                      \\epsilon_{ij}\n\n\n\n2. Estruturação e Importação do Arquivo de Dados\nA estrutura correta de um arquivo de dados para o caso de um experimento que utilizou um DBCA é ilustrada a seguir:\n\n\n\n\n\nsolucao\ndia\ny\n\n\n\n\nS1\nD1\n13\n\n\nS1\nD2\n22\n\n\nS1\nD3\n18\n\n\nS1\nD4\n39\n\n\nS2\nD1\n16\n\n\nS2\nD2\n24\n\n\nS2\nD3\n17\n\n\nS2\nD4\n44\n\n\nS3\nD1\n5\n\n\nS3\nD2\n4\n\n\nS3\nD3\n1\n\n\nS3\nD4\n22\n\n\n\n\n\n\n\nConsiderando que o script R que contém seus código para analisa os dados\nestão em uma pasta qualquer contendo uma subpasta denominada dado que contem seu arquivo de dados, para importar, por exemplo, um arquivo texto separado por tabulações denominado solucao.txt, pode-se utilizar o seguinte código\n\n# Pacotes utilizados\nlibrary(readr)\nlibrary(dplyr)\n\n# Importando os dados\ndados &lt;- read_tsv(\"dados/solucao.txt\", col_names = TRUE)\n\n# Verificando os dados\nglimpse(dados)\n\nRows: 12\nColumns: 3\n$ solucao &lt;chr&gt; \"S1\", \"S1\", \"S1\", \"S1\", \"S2\", \"S2\", \"S2\", \"S2\", \"S3\", \"S3\", \"S…\n$ dia     &lt;chr&gt; \"D1\", \"D2\", \"D3\", \"D4\", \"D1\", \"D2\", \"D3\", \"D4\", \"D1\", \"D2\", \"D…\n$ y       &lt;dbl&gt; 13, 22, 18, 39, 16, 24, 17, 44, 5, 4, 1, 22\n\n\nEm experimentos, em geral, fatores (ou tratamentos) são variáveis categóricas, como usamos a linguagem R, é melhor converter o tratamento (solucao) e os blocos (dia) para a classe factor para efetuar a análise dos dados. Essa conversão pode ser obtida usando a função transmute do pacote dplyr:\n\n# pacote utlizado\nlibrary(ggplot2)\n\ndados &lt;- dados %&gt;% transmute(solucao = as.factor(solucao),\n                             dia = as.factor(dia),\n                             y = as.numeric(y))\n\nglimpse(dados)\n\nRows: 12\nColumns: 3\n$ solucao &lt;fct&gt; S1, S1, S1, S1, S2, S2, S2, S2, S3, S3, S3, S3\n$ dia     &lt;fct&gt; D1, D2, D3, D4, D1, D2, D3, D4, D1, D2, D3, D4\n$ y       &lt;dbl&gt; 13, 22, 18, 39, 16, 24, 17, 44, 5, 4, 1, 22\n\n\nCom esses passsos, os dados foram importados e estruturados em um formato adequado para a análise.\n\n\n3. Faça uma Análise Exploratória dos Dados\n\n\n\n\n\n\nDica: Análise Exploratória dos Dados\n\n\n\nUma Análise Exploratória dos Dados com gráficos e estatísticas, é uma parte importante de qualquer análise para:\n\nentender as propriedades dos dados.\nestruturar os dados no formato adequado para análise.\nencontrar padrões nos dados.\ndescrever os dados/fenômeno.\nidentificar erros nos dados.\nsugerir estratégias de modelagem.\ndepurar análises.\ncomunicar resultados.\n\n\n\nUm boxplot comparativo das distribuições é o gráfico exploratório usual para explorar dados envolvendo uma variável numérica e uma ou mais variáveis categóricas.\nEntretanto, um boxplot requer uma amostra considerável (50 observações por grupo pelo menos), e caso seja utilizado em amostras pequenas, tende distorcer ou ocultar as propriedades da distribuição.\nPara amostras menores, como a do experimento em questão um gráfico de violino (violin plot) é mais adequado e tende a retratar melhor as propriedades das distribuições por grupo (no caso, por tratamento).\nPara produzir um gráfico de violino usando o pacote ggplot2, podemos utilizar a função geom_violin() e o seguinte código:\n\n# Violin plot entre y e solucao\n\nggplot(dados, aes(x = solucao, y = y)) + \n   geom_violin() +  \n   stat_summary(fun.y = \"median\", geom = \"point\", size = 2, color = \"red\") + \n   xlab(\"\") + \n   ylab(\"crescimento (quantidade) de bactérias\") + \n   geom_jitter(shape  = 16, position = position_jitter(0.2)) + \n   scale_x_discrete(labels = c(\"S1\" = \"Solução 1\", \n                               \"S2\" = \"Solução 2\",\n                               \"S3\" = \"Solução 3\")) +\n   theme_minimal()\n\n\n\n\n\n\n\nFig. 1. Gráfico de violino comparativo entre a medida de crescimento de bactérias e as soluções testadas. O ponto vermelho representa a mediana dos dados e os pontos pretos os dados.\n\n\n\n\n\nObservando a Figura 1, pode-se verificar que a solução 3, aparentemente, foi a que apresentou a maior redução no crescimento de bactérias nos containers de leite. Além disso, parece não haver diferença entre os resultados produzidos pelas soluções 1 e 2. Por fim, a Figura 1 mostra que as distribuições dos dados três soluções apresentam assimetria positiva devido a possíveis outliers nas distribuições dos resultados das três soluções.\n\n4. Estimação do Modelo Linear Normal.\nA estimação do modelo linear normal associado a um DBCA consiste, basicamente, na obtenção da tabela da Análise da Variância apropriada. No caso do DBCA d experimento que estamos analisando, podemos obtê-la com o seguinte código:\n\n# Estimação do Modelo com blocagem dos dias\nmod_dbca &lt;- aov(y ~ solucao + dia, data = dados)\nsummary(mod_dbca)\n\n            Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nsolucao      2  703.5   351.8   40.72 0.000323 ***\ndia          3 1106.9   369.0   42.71 0.000192 ***\nResiduals    6   51.8     8.6                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nA tabela da ANOVA resultante lista inicialmente os fatores analisados que estão sendo testadas no modelo, neste caso, temos solucao (tratamento ou fator de interesse) e dia (bloco ou fator de perturbação) e os resíduos do modelo (Residuals). Toda a variação que não é explicada pelos tratamentos e pelos blocos está incorporada na variância residual.\nA seguir descrevemos as colunas da tabela da ANOVA obtida:\n\nDf: A coluna Df exibe os graus de liberdade para a variável independente (o número de níveis na variável menos 1) e os graus de liberdade para os resíduos (o número total de observações menos um e menos o número de níveis nas variáveis independentes).\nSum Sq: A coluna Sum Sq exibe a partição da soma de quadrados total dos dados efetuada pela ANOVA.\nMean Sq: A coluna Mean Sq são os quadrados médios associados aos fatores em análise e aos resíduos, calculados dividindo-se a soma dos quadrados pelos respectivos graus de liberdade associados a cada fator e aos resíduos do modelo.\nF value: A coluna F value é a estatística de teste do teste F, obtida pela divisão do quadrado médio de cada fator pelo quadrado médio dos resíduos. Quanto maior o valor F, maior a probabilidade de que a variação causada pelos tratamentos seja real e não devida ao acaso, ou à variabilidade natural do fenômeno.\nPr(&gt;F): A coluna Pr(&gt;F) é o valor-p do teste F, que estima a probabilidade de obtermos a estatística do teste F calculada a partir dos dados ter ocorrido, se a hipótese nula de nenhuma diferença entre os médias dos tratamentos fosse verdadeira. Portanto, quanto menor o valor-p, em relação ao nível de significância adotado (em geral, \\alpha = 0.05), maior a chance de rejeitarmos a hipótese nula de que não há diferença entre as médias dos tratamentos, e vice-versa.\n\nComo o valor-p associado ao teste F entre as médias de contatem de bactérias das soluções testadas foi 0,0003, os dados fornecem forte evidência para rejeitarmos a hipótese nula de que as médias dos tratamentos são iguais.\nTamanho do Efeito\nTornou-se um padrão reportar tamanhos de efeitos em artigos científicos de diversas áreas, por diversas razões, para uma visão geral recomendo Cohen (1988), Fritz, Morris e Richler (2012) e Ferguson ([s.d.]). Para analisar uma visão de sua importâcnia em economica comportamental veja Hummel e Maedche (2019).\nPara ilustrar o conceito de tamanho do efeito para uma Análise da Variância, no contexto do experimento analisado, podemos perguntar inicialmente qual é a porcentagem da variação total no crescimento de bactérias associada aos tratamentos (soluções de lavagem). Esta medida é chamada de Eta-quadrático (\\eta^{2}):\n\n\\eta^2 = \\frac{SQ_{tratamento}}{SQ_{total}} = \\frac{703.5}{703.5 + 1106.9 + 51.8} =\n0.38 = 38\\%\n Podemos utilizar a função eta_squared() do pacote effectsize para estimar o \\eta^{2}:\n\n# Estimação dos tamanhos de efeitos\neffectsize::eta_squared(mod_dbca, partial = FALSE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 |       95% CI\n-------------------------------\nsolucao   | 0.38 | [0.00, 1.00]\ndia       | 0.59 | [0.00, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nQuando adicionamos mais termos ao modelo que estamos analisando (lembrando que no experimento que estamos anlisando temos um fator de interesse (tratamento) e um fator de perturbação (blocos)) podemos fazer uma pergunta diferente: qual a porcentagem da variação é explicada ao controlar quaisquer outros fatores.\n\n\\eta^{2}_p = \\frac{SQ_{tratamento}}{SQ_{tratamento} + SQ_{resíduo}} = \\frac{703.5}{703.5 + 51.8} = 0.93 = 93\\%\n\nEssa última pergunta é respondida pelo Eta-parcial quadrático \\eta^{2}_{p}, que é a porcentagem da variância parcial (após controlar outros fatores no modelo) associada a um fator.\nEm outras palavras, \\eta^{2}_{p} descreve a proporção de variação associada a um fator quando a variação associada a todos os outros efeitos incluídos no modeo foi removida.\nNovamente, podemos utiizar a função eta_squared(..., partial = TRUE) do pacote effectsize para estimar o \\eta^{2}_p:\n\n# Estimação dos tamanhos de efeitos\neffectsize::eta_squared(mod_dbca, partial = TRUE)\n\n# Effect Size for ANOVA (Type I)\n\nParameter | Eta2 (partial) |       95% CI\n-----------------------------------------\nsolucao   |           0.93 | [0.75, 1.00]\ndia       |           0.96 | [0.83, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nNa falta de conhecimento mais profundo e de outras referências, a regra de bolso (Cohen (1988)) para a interpretação da magnitude de \\eta^{2}_p é:\n\n\\eta^{2}_p \\approx 0.01 indica um efeito fraco ou pequeno.\n\\eta^{2}_p \\approx 0.06 indica um efeito moderado.\n\\eta^{2}_p \\approx 0.16 indica um efeito forte ou grande.\n\n\n\n\n5. Diagnóstico do modelo.\nUma das partes mais importantes da análise dos dados de um DBCA (e dos demais delineamentos), consiste em verificar se o modelo linear normal utilizado é adequado.\nEm geral, no caso de um DBCA, precisamos verificar:\n\nA validade da hipótese da normalidade aproximada dos resíduos do modelo,\nA validade da hipótese da variância dos resíduos ser aproximadamente constante (ou homogênea) em relação aos tratamentos.\nA validade da hipótese de aditividade do modelo linear normal, isto é, de que não há interação entre blocos e tratamentos.\n\n\n5.1 Verificação da normalidade dos resíduos\nÉ sempre importante analisar gráficos dos resíduos de modelos estatísticos, entretanto, infelizmente, não é usual reportar esses gráficos nos artigos científicos, prática que depoe contra o artigo e as práticas da revista.\nAlém da relevância, em si, de se analisar gráficos dos resíduos, sua importância deve-se também às conhecidas limitações dos testes de hipóteses formais comumente utilizados e reportados nas revistas.\nGrágico Quantil-Quantil dos Resíduos\nPodemos utilizar a função ggqqplot() do pacote ggpubr para produzirmos um gráfico quantil-quantil dos resíduos. Além dos resíduos, a função produz intervalos de confiança, caso os resíduos estejam dentro da região cinza, que representa os intervalos de confiança, temos evidência de que os resíduos seguem uma distribuição aproximadamente normal.\nPara o experimento em questão tempos:\n\n# pacote utilizado\nlibrary(ggpubr)\n\n# Gráfico quantil-quantil dos resíduos\nggqqplot(residuals(mod_dbca)) + \n  xlab(\"quantis teóricos de uma distribuição normal padrão\") + \n  ylab(\"resíduos\")\n\n\n\n\n\n\n\nFig. 2. Gráfico quantil-quantil para analisar a normalidade dos resíduos.\n\n\n\n\n\nAnalisando a Figura 2, é fácil verificar que os resíduos estão situados dentro da região de confiança (95%), portanto podemos considerar a distribuição dos resíduos compatível com uma distribuição normal padrão aproximada.\nTeste de Shapiro-Wilk de normalidade dos resíduos\nUm dos testes mais comumente utlizados para testar formalmente se os resíduos de um modelo linear experimental são aproximadamente normais é o teste de Shapiro-Wilk.\nDe forma simplificada, podemos dizer que a hipótese nula do teste é que os resíduos são, aproximadamente, normalmente distribuídos. Assim, se o valor-p do teste for menor que o nível de significância escolhido (\\alpha), então a hipótese nula é rejeitada e há evidências de que os resíduos testados não são normalmente distribuídos.\nPor outro lado, se o valor-p for maior que o nível de significância escolhido, a hipótese nula (de que os dados vieram de uma população normalmente distribuída) não pode ser rejeitada. Por exemplo, para um nível \\alpha de 0,05, um valor p inferior a 0,05 implica na rejeição da hipótese nula de que os dados são de uma população com distribuição (aproximadamente) normalmente distribuída.\nPara o experimento que estamos analisando, podemos executar o teste de Shapiro-Wilk utilizando a função interna shapiro.test():\n\n#### Teste de Shapiro-Wilks\nshapiro.test(mod_dbca$residuals)\n\n\n    Shapiro-Wilk normality test\n\ndata:  mod_dbca$residuals\nW = 0.93208, p-value = 0.4027\n\n\nConsiderando \\alpha = 0.05, como o valor-p do teste é muito superior ao nível de significância escolhido, os dados fornecem forte evidência para não rejeitarmos a hipótese nula de que os resíduos seguem uma distribuição aproximadamente normal.\n\n\n5.2 Verificação da constância aproximada da variância dos resíduos\nPara testar a hipóteses de constância da variância dos resíduos entre os níveis dos tratamentos, as três soluções no caso, recomendo utilizar a função interna fligner.test() que implementa o teste de constância da variância de Fligner-Killeen.\nA recomendação deve-se ao fato de que o teste Fligner-Killeen foi considerado, em um estudo de simulação, como um dos testes de homogeneidade de variâncias mais robustos contra desvios da normalidade, ver Conover, Johnson & Johnson (1981).\n\n#### Teste de Bartlett: variancia homogenea pelo tratamento\nfligner.test(y ~ solucao, data = dados)\n\n\n    Fligner-Killeen test of homogeneity of variances\n\ndata:  y by solucao\nFligner-Killeen:med chi-squared = 1.4915, df = 2, p-value = 0.4744\n\n\nConsiderando \\alpha = 0.05, como o valor-p do teste é muito superior ao nível de significância escolhido, os dados fornecem forte evidência para não rejeitarmos a hipótese nula de que os resíduos possuem variância constante (ou homogênea) entre as diferentes soluções (tratamentos) testadas.\n\n\n5.3 Verificando se há interação entre bloco e tratamento\nO modelo estatístico linear normal utilizado para um DBCA:\n\ny_{ij} = \\mu + \\tau_i + \\beta_j + \\epsilon_{ij},\n\né completamente aditivo, isto é, os efeitos devidos aos tratamentos e aos blocos são adicionados, e não multiplicados, por exemplo. Apesar deste modelo aditivo simples ser frequentemente útil, há situações nas quais ele é inadequado.\nEspecificamente, o modelo aditivo é inadequado em situações nas quais há interação entre tratamentos e blocos. Interações podem ocorrer, por exemplo, quando a variável resposta (y_{ij}) é medida na escala errada. Assim, se uma relação que é multiplicativa na escala original, digamos:\n\nE(y_{ij}) = \\mu \\tau_i \\beta_j\n\nela pode ser tornada aditiva (ou linear) utilizando uma escala logarítimica:\n\n\\ln(E(y_{ij})) = \\ln(\\mu) + \\ln(\\tau_i) + \\ln(\\beta_j)\n\nEste tipo de interação pode ser eliminada pela transformação logaritmica, entretanto, nem todas as interações pode ser tratadas tão facilmente.\nSe interações estão presentes, elas podem afetar seriamente e até invalidar a Análise da Variância de um experimento. Em geral, a presença de interações, infla o quadrado médio do resíduo e podem afetar adversamente a comparação entre as médias dos tratamentos.\nQuando ambos os fatores sào de interesse (o que não é o caso em um DBCA), pode-se utilizar delineamentos fatoriais para incorporar o efeito de interações.\nHá métodos gráficos e o teste de hipóteses formal de aditividade de Tukey para verificar a presença de interação.\nMétodo Gráfico\nUm gráfico de interação exibe a média (ou outra estatística) da variável resposta contra combinações dos fatores (blocos e tratamentos), ilustrando assim possíveis interações. A função interna interaction.plot() da linguagem R pode ser utilizad para produzir este gráfico, a sintáxe da função para o exeperimento que estamos analisando é:\n\n# Verificando Interação entre Blocos e níveis do Fator (solucao)\ninteraction.plot(dados$dia, dados$solucao, dados$y, fixed = TRUE)   \n\n\n\n\n\n\n\nFig. 3. Gráfico de interação entre a média da variável resposta e combinaçòes das soluções (tratamento) e dias (blocos).\n\n\n\n\n\nIdealmente, as linhas de um gráfico de interação devem ser paralelas, ou seja, não devem se cruzar. Apesar da Figura 3 exibir um cruzamento, não parece haver uma interação relevante entre as solucões (tratamentos) e os dias (blocos). Uma limitaçào desse gráfico é que ele reflete possíveis imprecisões (alta variabilidade) nas estimativas das médias, o que parece ser o caso nesse experimento devido ao pequeno tamanho da amostra e à possível presença de outliers.\nTeste de Aditividade de Tukey\nO teste de aditividade de Tukey testa as seguintes hipóteses:\n\n\\begin{cases}\nH_o:  & \\text{efeitos principais e blocos são aditivos,}  \\\\\nH_a:  & \\text{efeitos principais e blocos são não-aditivos}\n\\end{cases}\n Podemos utilizar a função tukey.add.test(variável resposta, tratamentos, blocos) do pacote asbio para executar esse teste. No caso do experimento em análise temos:\n\n# Testa H0: Não há efeito de interação\nasbio::tukey.add.test(dados$y, dados$solucao, dados$dia)\n\n\nTukey's one df test for additivity \nF = 2.7732343   Denom df = 5    p-value = 0.1567331\n\n\nComo o valor-p do teste (0.16) é maior que \\alpha = 0.05, não rejeitamos a hipótese nula de que os efeitos principais (\\tau_i) e os blocos (\\beta_j) são aditivos, isto é, o resultado do teste fornece evidência de que não há interação entre blocos e tratamento.\n\n\n\n6. Os resultados indicaram diferenças entre os crescimentos médios de bactérias produzidos pelas três soluções?\nUma Análise da Variância dos resultados do delineamento em blocos completos executado foi conduzida para comparar o efeito de três diferentes soluções de lavagem sobre o crescimento de bactérias em containers de leite. A ANOVA indicou que houve uma diferença entre pelo menos duas soluções [ F(2, 6) = 40.72, p &lt; .001, \\eta^2_p = .93]. A análise dos resíduos indicou a adequação do modelo utilizado.\n\n\n7. Qual solução deveria ser recomendada? Utilize os testes de Fisher e Tukey.**\nComo a ANOVA indicou diferenças entre o crescimento (quantidade) média de bactérias entre as soluções testadas, dos laboratórios, surgem outras perguntas:\n\nQuais soluçóes apresentaram médias estatisticamente iguais e quais apresentaram médias estatisticamente iguais?\nQual solução apresentou o melhor resultado? Ou seja, apresentou a maior redução no crescimento de bactérias nos containers de leite?\nNecessitamos de procedimentos de comparações múltiplas de médias para responder estas questões.\n\nEntre os dois mais utilizados, temos os testes de Fisher e de Tukey.\nTeste de Fisher\nDuas médias de tratamentos são estatisticamente diferentes segundo o teste da diferença mínima significativa (dms) de Fisher se:\n\n|\\bar{y}_i - \\bar{y}_j| \\geq dms = t_{(\\alpha, gl_{resíduos})} \\sqrt{\\frac{2QM_{resíduos}}{J}}\n\nsendo gl_{resíduos} o grau de liberdade do resíduo:\n\nO teste de Fisher requer que o teste F da ANAVA tenha rejeitado a hipótese nula de igualdade entre as médias de tratamentos de forma a controlar o erro tipo I\nEste teste é liberal, no sentido de que possui grande poder e erro tipo I maior que um teste mais conservador.\n\n\n### Teste de Fisher\nagricolae::LSD.test(mod_dbca, \n                    \"solucao\", \n                    alpha = 0.05, \n                    p.adj = \"fdr\", \n                    console = TRUE)\n\n\nStudy: mod_dbca ~ \"solucao\"\n\nLSD t Test for y \nP value adjustment method: fdr \n\nMean Square Error:  8.638889 \n\nsolucao,  means and individual ( 95 %) CI\n\n       y       std r       se      LCL      UCL Min Max   Q25  Q50   Q75\nS1 23.00 11.284207 4 1.469599 19.40402 26.59598  13  39 16.75 20.0 26.25\nS2 25.25 12.996794 4 1.469599 21.65402 28.84598  16  44 16.75 20.5 29.00\nS3  8.00  9.486833 4 1.469599  4.40402 11.59598   1  22  3.25  4.5  9.25\n\nAlpha: 0.05 ; DF Error: 6\nCritical Value of t: 2.446912 \n\nMinimum Significant Difference: 5.085484 \n\nTreatments with the same letter are not significantly different.\n\n       y groups\nS2 25.25      a\nS1 23.00      a\nS3  8.00      b\n\n\nTeste de Tukey\nDuas médias de tratamentos são estatisticamente diferentes segundo o teste de Tukey se:\n\n|\\bar{y}_i - \\bar{y}_j| \\geq dhs = q_{(\\alpha, I, gl_{resíduos})} \\sqrt{\\frac{QM_{resíduos}}{J}}\n\nEste teste é muito conservador, ou seja tem o menor poder em relação a testes liberais e o menor erro tipo I.\n\n### Teste de Tukey\nTukeyHSD(mod_dbca, \n         \"solucao\", \n         conf.level = 0.95)\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = y ~ solucao + dia, data = dados)\n\n$solucao\n        diff        lwr        upr     p adj\nS2-S1   2.25  -4.126879   8.626879 0.5577862\nS3-S1 -15.00 -21.376879  -8.623121 0.0008758\nS3-S2 -17.25 -23.626879 -10.873121 0.0004067\n\n\nNo caso do experimento analisado, os dois testes produziram os mesmos resultados:\n\no crescimento médio de bactérias devido às soluções 1 e 2 são estatisticamente iguais. Médias identificadas com a mesma letra representam esse fato, apresentam a letra a no caso.\no crescimento médio de bactérias produzido pela solução 3 é estatisticamente diferente das médias produzidas pelas soluções 1 e 2, repare que apenas a média produzida pela solução 3 contém a letra b.\n\nPortanto, a solução recomendada, com fundamento nos resultados do experimento, seria a solução 3.\n\n\n8. Comunicando os resultados finais da análise.\nA forma de reportar análises estatísticas depende da área do conhecimento e da revista científica específica. Entretanto, o padrão definido pelo American Psychological Association (Cooper (2018)) é bastante utilizado em ciëncias comportamentais. Seguindo o padrão APA, as conclusões da análise do experimento seriam aproximadamente como se segue:\nUma Análise da Variância dos resultados do delineamento em blocos completos executado foi conduzida para comparar o efeito de três diferentes soluções de lavagem sobre o crescimento de bactérias em containers de leite. A ANOVA indicou que houve uma diferença entre pelo menos duas soluções [F(2, 6) = 40.72, p &lt; .001, \\eta^2_p = .93]. A análise dos resíduos indicou a adequação do modelo utilizado.\nComparações post hoc usando o teste de Tukey indicaram que o crescimento médio de bactérias após a aplicação da solução 3 foi significativamente diferente dos resultados obtidos com a solução 1 [p &lt; 0.001, IC 95%. = [-21,38, -8,62] e com a solução 2 [p &lt; 0.001, IC 95%. = [-23,63, -10,87].\nNão houve diferença estatisticamente significativa entre o crescimento médio de bactérias produzidos pelas soluções 1 e 2 [p &lt; 0,5578, IC 95%. = [-4,13, -8,63].\n\n\n9. Utilize um Método Não-Paramétrico\nEm construção"
  },
  {
    "objectID": "tutorials/dbca/index.html#referências",
    "href": "tutorials/dbca/index.html#referências",
    "title": "Delineamento em Blocos Completos Aleatorizados (DBCA)",
    "section": "Referências",
    "text": "Referências\n\n\nCASELLA, G. Statistical Design. [s.l.] Springer, 2008.\n\n\nCOHEN, J. Statistical power analysis for the behavioral sciences. 2. ed. [s.l.] Routledge, 1988.\n\n\nCOOPER, H. Reporting quantitative research in psychology: How to meet APA style journal article reporting standards. [s.l.] American Psychological Association, 2018.\n\n\nFERGUSON, C. J. Methodological issues and strategies in clinical research. Em: KAZDIN, A. E. (Ed.). [s.l.] American Psychological Association, [s.d.].\n\n\nFRITZ, C. O.; MORRIS, P. E.; RICHLER, J. J. Effect size estimates: current use, calculations, and interpretation. Journal of Experimental Psychology: General, v. 141, n. 1, p. 2, 2012.\n\n\nHUMMEL, D.; MAEDCHE, A. How effective is nudging? A quantitative review on the effect sizes and limits of empirical nudging studies. Journal of Behavioral and Experimental Economics, v. 80, p. 47–58, 2019.\n\n\nMEIER, L. ANOVA and Mixed Models: A Short Introduction Using R. [s.l.] CRC Press, 2022.\n\n\nMONTGOMERY, D. C. Design and Analysis of Experiments. 5. ed. [s.l.] John Wiley & sons, 2001."
  },
  {
    "objectID": "publications/index.html",
    "href": "publications/index.html",
    "title": "Published Papers",
    "section": "",
    "text": "Published Papers\n\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n    Washington Santos da Silva, Bruno César de Melo Moreira, Raquel Aparecida Soares Franco. (2024) Efeito de cotas para ingresso e de fatores socioeconômicos sobre o desempenho acadêmico de estudantes em um curso técnico integrado: um estudo de caso. Revista Eletrônica de Educação, 18(1).\n  \n\n  \n    Lélis Pedro de Andrade, Washington Santos da Silva, Daniel Fonseca Costa, Bruno César de Melo Moreira, Adriano Olímpio Tonelli. (2024) Financial Flexibility and Stock Return of Brazilian Companies: Evidence During the COVID-19 Crisis. Revista Evidenciação Contábil & Finanças, 11(2).\n  \n\n  \n    Natan Felipe Silva, Lélis Pedro de Andrade, Washington Santos da Silva, Maísa Kely de Melo, Adriano Olímpio Tonelli. (2024) Portfolio optimization based on the pre-selection of stocks by the Support Vector Machine model. Finance Research Letters, 61,  105014.\n  \n\n  \n    Daniel Fonseca Costa, Bruno César de Melo Moreira, Washington Santos da Silva, Lélis Pedro de Andrade. (2023) Overconfidence in Managerial Decision-Making among Brazilian Accountants and Managers: An Experimental Study.  Business Management and Strategy, 14:2, 39-70.\n  \n\n  \n    Taynara Cardoso Ribeiro, Lélis Pedro Andrade, Washington Santos da Silva, Bruno César de Melo Moreira, Daniel Fonseca Costa. (2021) Determinantes pelo Prêmio pelo Direito de Voto no Mercado de Ações Brasileiro. Revista de Gestão, Finanças e Contabilidade, 11:2, 4-21.\n  \n\n  \n    Thiago Augusto da Costa Silva, Marcos de Paula Jr., Washington Santos da Silva, Gustavo Augusto Lacorte. (2021) Can moderate heavy metal soil contaminations due to cement production influence the surrounding soil bacterial communities?. Ecotoxicology, 31, 134-148.\n  \n\n  \n    Thiago Augusto da Costa Silva, Marcos de Paula Jr., Washington Santos da Silva, Gustavo Augusto Lacorte. (2021) Deposition of Potentially Toxic Metals in the Soil from Surrounding Cement Plants in a Karst Area of Southeastern Brazil. Conservation, 1(3), 137-150.\n  \n\n  \n    Daniel Fonseca Costa, Bruno César de Melo Moreira, Francisval de Melo Carvalho, Washington Santos da Silva. (2020) Anchoring effect in managerial decision-making in accountants and managers: an experimental study. Revista Brasileira de Estratégia, 11:3, 425-445.\n  \n\n  \n    Daniel Fonseca Costa, Francisval de Melo Carvalho, Bruno César de Melo Moreira, Washington Santos da Silva. (2020) Viés de confirmação na tomada de decisão gerencial: um estudo experimental com gestores e contadores. Revista de Contabilidade e Organizações, 11:2, 4-21.\n  \n\n  \n    Nayara Teixeira Santos, Gisele Tessari Santos, Washington Santos da Silva, Wanyr Romero Ferreira (2020) A System Dynamics Model for Sales and Operations Planning: An Integrated Analysis for the Lime Industry. International Journal of System Dynamics Applications, 9:1, 1-17.\n  \n\n  \n    Natália Duarte de Medeiros, Francisval de Melo Carvalho, Caio Peixoto Chain, Gideon Carvalho de Benedicto, Washington Santos da Silva (2018) Capital Structure and Information Asymmetry: A Study of Brazilian Publicly Traded Companies of Testile and Electricity Industries. Revista de Administração da Universidade Federal de Santa Maria, 11:2, 268-289.\n  \n\n  \n    Cassia M. B. Nobre, Roberto A. Braga. Jr, Antônio G. Costa, R. R. Cardoso, Washington Santos da Silva, Thelma Sáfadi. (2009) Biospeckle laser spectral analysis under Inertia Moment, Entropy and Cross-Spectrum methods. Optics Communications, 282:11, 2236-2242.\n  \n\n  \n    Roberto A. Braga. Jr, Washington Santos da Silva, Thelma Sáfadi, Cassia M. B. Nobre (2008) Time history speckle pattern under statistical view. Optics Communications, 281:9, 2443-2448.\n  \n\n  \n    Wanderci Alves Bitencourt, Washington Santos Silva, Thelma Sáfadi. (2006) Hedge dinâmicos: Uma evidência para os contratos futuros brasileiros. Organizações Rurais & Agroindustriais, 8:1, 71-78.\n  \n\n  \n    Washington Santos da Silva, Thelma Sáfadi, Luiz Gonzaga de Castro Júnior. (2005) Uma análise empírica da volatilidade do retorno de commodities agrícolas utilizando modelos ARCH: os casos do café e da soja. Revista de Economia e Sociologia Rural, 43:1, 119-134.\n  \n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/p01/tempo.html",
    "href": "blog/p01/tempo.html",
    "title": "Tempo",
    "section": "",
    "text": "“O problema é que você acha que tem tempo”\n— Jack Kornfield"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Tempo\n\n\n\n\n\n\n\n\n\n\n\n28 fevereiro 2025\n\n\n\n\n\n\nNenhum item correspondente"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Washington S. da Silva",
    "section": "",
    "text": "Curriculum Lattes  ORCID  Google Scholar\n\n\n\nI am a Full Professor at IFMG - Campus Formiga. My research interests include Applied Statistics and Econometrics, Data Science, Reproducible Research, Time Series Analysis, Statistical and Machine Learning Applied to Finance, and Bayesian Econometrics."
  },
  {
    "objectID": "teaching/index.html#introdução-à-ciência-dos-dados-1",
    "href": "teaching/index.html#introdução-à-ciência-dos-dados-1",
    "title": "Graduate Courses",
    "section": "Introdução à Ciência dos Dados",
    "text": "Introdução à Ciência dos Dados"
  },
  {
    "objectID": "teaching/index.html#métodos-quantitativos-aplicados-à-administração",
    "href": "teaching/index.html#métodos-quantitativos-aplicados-à-administração",
    "title": "Graduate Courses",
    "section": "Métodos Quantitativos Aplicados à Administração",
    "text": "Métodos Quantitativos Aplicados à Administração"
  },
  {
    "objectID": "teaching/index.html#visualização-de-dados",
    "href": "teaching/index.html#visualização-de-dados",
    "title": "Graduate Courses",
    "section": "Visualização de Dados",
    "text": "Visualização de Dados"
  },
  {
    "objectID": "teaching/index.html#análise-multivariada",
    "href": "teaching/index.html#análise-multivariada",
    "title": "Graduate Courses",
    "section": "Análise Multivariada",
    "text": "Análise Multivariada"
  },
  {
    "objectID": "teaching/index.html#previsão-de-séries-temporais",
    "href": "teaching/index.html#previsão-de-séries-temporais",
    "title": "Graduate Courses",
    "section": "Previsão de Séries Temporais",
    "text": "Previsão de Séries Temporais"
  },
  {
    "objectID": "tutorials/git/index.html",
    "href": "tutorials/git/index.html",
    "title": "Controle de Versão com Git e GitHub",
    "section": "",
    "text": "Objetivos\n\n\n\n\nEste tutorial é destinado aos estudantes do bacharelado em Administração e aos mestrandos do Mestrado Profissional em Administração do IFMG - Campus Formiga.\nTambém é útil para administradores, economistas, contabilistas, auditores e outros profissionais que desejam utilizar ferramentas modernas para criar relatórios e outros produtos de forma reproduzível e auditável.\nVocê não precisa ser programador para aproveitar este material. O tutorial foi elaborado pensando em profissionais de negócios que desejam melhorar seu fluxo de trabalho com documentos, análises e projetos.\n\n\n\n\n\n\n\n\n\nO que é Git?\n\n\n\n\nImagine o Git como um “sistema de salvamento inteligente” que registra cada versão dos seus documentos e projetos.\nAo contrário do método tradicional de salvar vários arquivos como “Relatório_v1.docx”, “Relatório_v2.docx”, o Git guarda apenas as mudanças, economizando espaço.\nÉ como uma “máquina do tempo” para seu trabalho: você pode visualizar ou restaurar qualquer versão anterior quando precisar, sem perder as versões mais recentes.\nFacilita o trabalho em equipe, permitindo que várias pessoas alterem os mesmos arquivos sem conflitos graves.\n\n\n\n\n\n\n\n\n\nPor que usar Git?\n\n\n\n\nEvita a confusão de múltiplas versões: Adeus a arquivos como\n“Relatorio_Final_v2_Revisado_Corrigido.docx”.\nRecuperação de trabalho: Se algo der errado, você pode voltar facilmente a uma versão anterior que funcionava.\nExperimente sem medo: Tente novas abordagens sabendo que pode reverter se não gostar do resultado.\nTrabalho colaborativo organizado: Múltiplas pessoas podem trabalhar no mesmo projeto sem sobrescrever o trabalho umas das outras.\nDocumentação automática: O histórico de alterações serve como documentação da evolução do seu trabalho.\n\n\n\n\n\n\n\n\n\nO que é GitHub?\n\n\n\n\nUm serviço online que funciona como uma “nuvem para projetos Git”.\nPense no GitHub como um Google Drive ou Dropbox especializado para projetos que usam Git.\nAlém de armazenar seus arquivos, oferece ferramentas para colaboração, revisão e discussão do trabalho.\nÉ amplamente usado tanto por desenvolvedores quanto por pesquisadores, analistas e educadores para compartilhar trabalhos.\n\n\n\n\n\n\n\n\n\nPor que usar GitHub?\n\n\n\n\nBackup seguro: Seus projetos ficam armazenados na nuvem, protegidos contra perda de dados se seu computador falhar ou for perdido.\nPortfólio profissional: Muitos profissionais usam o GitHub como vitrine de seus trabalhos e habilidades.\nColaboração simplificada: Facilita o trabalho em equipe mesmo com pessoas em locais diferentes.\nCompartilhamento eficiente: Compartilhe seu trabalho com colegas de curso, de trabalho ou com o mundo.\nAprendizado constante: Acesse e estude projetos semelhantes ao seu para aprender novas técnicas e abordagens."
  },
  {
    "objectID": "tutorials/git/index.html#configuração-inicial",
    "href": "tutorials/git/index.html#configuração-inicial",
    "title": "Controle de Versão com Git e GitHub",
    "section": "5.1 Configuração Inicial",
    "text": "5.1 Configuração Inicial\n\n\n\n\n\n\nConfiguração Inicial - Orientando\n\n\n\nEsta etapa será realizada apenas uma vez, no início do trabalho com seu orientador:\n\nCrie o repositório do projeto:\n\n\nCrie o repositório no GitHub como visto anteriormente.\nAdicione seu orientador como colaborador.\n\n\nClone o repositório para seu computador:\n\ngit clone https://github.com/nome-usuario/nome-repositorio.git\nEste comando cria uma cópia local do repositório em seu computador.\n\nConfirme que está tudo funcionando:\n\n\nEntre na pasta criada: cd nome-repositorio.\nVerifique a conexão: git remote -v (deve mostrar a URL do GitHub).\nFaça um teste simples:\n\n# Crie um arquivo de teste\necho \"# Teste de colaboração\" &gt; teste.md\n\n# Adicione e faça commit\ngit add teste.md\ngit commit -m \"Teste inicial de colaboração\"\n\n# Envie para o GitHub\ngit push origin main\n\nVerifique no GitHub se o arquivo apareceu"
  },
  {
    "objectID": "tutorials/git/index.html#ciclo-de-desenvolvimento",
    "href": "tutorials/git/index.html#ciclo-de-desenvolvimento",
    "title": "Controle de Versão com Git e GitHub",
    "section": "5.2 Ciclo de Desenvolvimento",
    "text": "5.2 Ciclo de Desenvolvimento\n\n\n\n\n\n\nCiclo Diário/Semanal de Trabalho - Orientando\n\n\n\nEste é o ciclo que você repetirá regularmente enquanto trabalha em seu projeto:\n\nComece o dia de trabalho atualizando seu repositório local:\n\n# Certifique-se de estar na pasta do projeto\ngit pull origin main\nEste comando baixa qualquer mudança que seu orientador possa ter feito.\n\nTrabalhe em seus arquivos normalmente:\n\n\nEscreva seu capítulo da dissertação\nRealize análises de dados\nCrie visualizações ou tabelas\nRedija seções do artigo\n\nTrabalhe como você normalmente faria em seu computador.\n\nSalve seu progresso :\n\nA cada conquista significativa ou no final do dia:\n# Verifique o que mudou\ngit status\n\n# Veja as alterações específicas (opcional)\ngit diff\n\n# Adicione os arquivos modificados\ngit add .\n\n# Salve as mudanças localmente com uma mensagem descritiva\ngit commit -m \"Adiciona análise descritiva no capítulo 3\"\n\n# Envie para o GitHub\ngit push origin main\nBoas práticas para as mensagens de commit:\n\nSeja específico: “Reescreve o texto da seção Introdução”.\nUse verbos no presente: “Adiciona”, “Corrige”, “Atualiza”.\nIndique onde a mudança foi feita: “Revisão da Literatura”, “Metodologia”, “Resultados e Discussão”, “Introdução” etc."
  },
  {
    "objectID": "tutorials/git/index.html#incorporação-de-feedback",
    "href": "tutorials/git/index.html#incorporação-de-feedback",
    "title": "Controle de Versão com Git e GitHub",
    "section": "5.3 Incorporação de Feedback",
    "text": "5.3 Incorporação de Feedback\n\n\n\n\n\n\nQuando seu Orientador Fornecer Feedback - Orientando\n\n\n\nEste processo ocorre depois que seu orientador revisar seu trabalho e fizer alterações:\n\nObtenha as mudanças feitas pelo orientador:\n\ngit pull origin main\nIsso baixa as alterações, comentários ou correções feitas pelo orientador.\n\nRevise as mudanças:\n\n# Para ver quais arquivos foram modificados pelo orientador\ngit log\n\n# Para ver as mudanças específicas em um arquivo\ngit diff HEAD~1 HEAD -- caminho/do/arquivo\nEste comando mostra o que mudou entre a versão atual e a anterior.\n\nTrabalhe com as sugestões:\n\n\nImplemente as correções sugeridas.\nResponda a questionamentos (pode ser no próprio documento ou em um comentário no GitHub).\nComplemente seções conforme solicitado.\n\n\nContinue o ciclo normal:\n\nVolte ao passo 2 (Ciclo de Desenvolvimento) e continue seu trabalho."
  },
  {
    "objectID": "tutorials/git/index.html#lidando-com-conflitos",
    "href": "tutorials/git/index.html#lidando-com-conflitos",
    "title": "Controle de Versão com Git e GitHub",
    "section": "5.4 Lidando com Conflitos",
    "text": "5.4 Lidando com Conflitos\n\n\n\n\n\n\nE se ocorrerem conflitos?\n\n\n\nUm conflito acontece quando você e seu orientador editam a mesma parte de um arquivo. Não se assuste, isso é normal e pode ser resolvido:\n\nQuando ocorre um conflito:\n\nAo executar git pull, você verá uma mensagem como:\nCONFLICT (content): Merge conflict in nome-do-arquivo.extensao\n\nAbra o arquivo com conflito:\n\nVocê verá marcações como estas:\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nSeu texto (versão local)\n=======\nTexto do orientador (versão do GitHub)\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; abcd1234\n\nResolva o conflito manualmente:\n\n\nEdite o arquivo para conter o conteúdo final desejado\nRemova as marcações (&lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, &gt;&gt;&gt;&gt;&gt;&gt;&gt;)\nSalve o arquivo\n\n\nConclua a resolução:\n\n# Adicione o arquivo resolvido\ngit add nome-do-arquivo\n\n# Faça um commit da resolução\ngit commit -m \"Resolve conflito no arquivo X\"\n\n# Envie para o GitHub\ngit push origin main\nDica: Comunique-se com seu orientador quando encontrar conflitos. Às vezes, uma conversa rápida pode esclarecer qual versão deve ser mantida."
  },
  {
    "objectID": "tutorials/git/index.html#trabalhando-com-issues-do-github",
    "href": "tutorials/git/index.html#trabalhando-com-issues-do-github",
    "title": "Controle de Versão com Git e GitHub",
    "section": "5.5 Trabalhando com Issues do GitHub",
    "text": "5.5 Trabalhando com Issues do GitHub\n\n\n\n\n\n\nUsando Issues para Organizar Tarefas\n\n\n\nO GitHub possui uma ferramenta chamada “Issues” que é excelente para:\n\nListar tarefas pendentes\nRegistrar problemas a serem resolvidos\nDocumentar discussões entre orientando e orientador\n\nComo usar:\n\nCrie uma issue:\n\n\nNo GitHub, acesse a aba “Issues” do repositório.\nClique em “New issue”.\nDê um título descritivo e explique a tarefa/problema.\nUse marcadores como listas, negrito e itálico para organizar.\nAdicione etiquetas (labels) como “revisão”, “urgent”, “discussão”.\n\n\nAcompanhe o progresso:\n\n\nComente nas issues para atualizar o progresso.\nReferencie commits relacionados usando “#” (exemplo: “Resolve #4”).\nFeche a issue quando a tarefa estiver concluída.\n\n\nReferencie issues nos commits:\n\ngit commit -m \"Adiciona análise de correlação solicitada na issue #5\"\nIsso cria uma referência cruzada que facilita acompanhar o histórico de mudanças e discussões."
  },
  {
    "objectID": "tutorials/git/index.html#aplicando-este-workflow-a-outros-cenários",
    "href": "tutorials/git/index.html#aplicando-este-workflow-a-outros-cenários",
    "title": "Controle de Versão com Git e GitHub",
    "section": "5.6 Aplicando este Workflow a Outros Cenários",
    "text": "5.6 Aplicando este Workflow a Outros Cenários\n\n\n\n\n\n\nAdaptando para Outras Colaborações\n\n\n\nEste mesmo fluxo de trabalho pode ser facilmente adaptado para colaborar com:\n\nColegas de classe em projetos em grupo.\nOutros pesquisadores em artigos científicos.\nEquipes de trabalho em relatórios ou análises.\nProjetos de extensão ou iniciação científica.\n\nA mesma lógica se aplica:\n\nConfiguração inicial (uma vez).\nCiclo frequente de atualizações.\nIncorporação de feedback dos colaboradores.\n\nA diferença principal em outros cenários pode ser a necessidade de usar “branches” (ramificações) para trabalhos mais complexos com múltiplos colaboradores, um tópico que pode ser explorado quando você já estiver confortável com este fluxo básico."
  },
  {
    "objectID": "tutorials/git/index.html#resumo-do-workflow",
    "href": "tutorials/git/index.html#resumo-do-workflow",
    "title": "Controle de Versão com Git e GitHub",
    "section": "5.7 Resumo do Workflow",
    "text": "5.7 Resumo do Workflow\n\n\n\n\n\n\nResumindo\n\n\n\nO ciclo de trabalho com seu orientador segue este padrão:\n\nComece o dia puxando alterações: git pull origin main\nTrabalhe nos arquivos normalmente\nVerifique mudanças: git status e git diff\nSalve seu progresso:\ngit add .\ngit commit -m \"Mensagem descritiva\"\ngit push origin main\nRepita diariamente ou quando concluir uma parte significativa\n\nLembre-se: a comunicação clara com seu orientador sobre qual parte cada um está trabalhando pode evitar conflitos e retrabalho."
  },
  {
    "objectID": "Readme.html",
    "href": "Readme.html",
    "title": "Washington S. da Silva",
    "section": "",
    "text": "My personal home page.\nThe page uses the template provided by Prof. Rob Hyndman."
  },
  {
    "objectID": "publications/arch.html",
    "href": "publications/arch.html",
    "title": "Uma análise empírica da volatilidade do retorno de commodities agrícolas utilizando modelos ARCH: os casos do café e da soja",
    "section": "",
    "text": "Abstract\nWe examined the volatility process of the returns of two important Brazilian agricultural commodities, coffee and soy, using ARCH class models. Empirical results suggest strong signs of persistence and asymmetry in the volatility of both series. Furthermore, the results suggest that the design of policies that create, facilitate the access and stimulate the use of market-based hedging devices can be proper strategies for such sectors in view of the persistence of shocks and the pronounced volatility found for the returns of these commodities."
  },
  {
    "objectID": "publications/efeito_cotas.html",
    "href": "publications/efeito_cotas.html",
    "title": "Efeito de cotas para ingresso e de fatores socioeconômicos sobre o desempenho acadêmico de estudantes em um curso técnico integrado: um estudo de caso",
    "section": "",
    "text": "Abstract\nThis study aims identify the determinants of premium for voting rights in the Brazilian market. To this end, we developed a dataset consisted of 383 companies listed on the Brasil, Bolsa, Balcão (B3) from 2000 to 2015, to use the fixed effects regressions with panel data models. We identified that the average of the premium found for the period is 18.07%, however, this value is close to 40.00% in years considered of systemic crisis, as occurred in 2008 and 2015. In addition to the positive influence of the years of decline in the stock market in the value of the premium, the results showed that i) the increase in liquidity of shares with voting rights negatively affect the value of the premium for control; and ii) the level of liabilities payable in relation to assets has an inverted U-shape in relation to the voting premium. The study contributes to the literature by verifying that a structure of two classes of shares allows the accentuation of the premium for voting rights in periods of systematic crisis."
  },
  {
    "objectID": "publications/flexibilidade.html",
    "href": "publications/flexibilidade.html",
    "title": "Financial Flexibility and Stock Return of Brazilian Companies: Evidence During the COVID-19 Crisis",
    "section": "",
    "text": "Abstract\nObjective: The objective was to verify the effect of financial flexibility on the value of shares of Brazilian companies during the pandemic period caused by COVID-19.\nBackground: Financial flexibility is associated with the ability of a company to face financial diffi-culties, but there are still no studies in Brazil that show its relationship with the value of shares in periods with a shock to companies’ revenues.\nMethod: Regression analysis was applied to a cross-section of data with 102 companies listed on the Brazil Broad Index (IBRA). Two economic periods of the pandemic were considered, the col-lapse, between 2/2/2020 and 3/23/2020, and the stimulus to the economy, on 3/24/2020.\nResults: The evidence show that in the period of economic collapse, the financial flexibility due to debt capacity proved to be valuable for companies, especially for those most affected by COVID-19, which had more pronounced reductions in revenues in the first half of 2020. We have also ob-served that the level of cash retention had no impact on stock returns during the collapse period. On the date of economic stimulus, the companies’ cash level showed a positive relationship with the value of shares, but there were no evidence that this appreciation occurred for companies with more affected revenues.\nContributions: This study contributes to the literature by verifying the impact of financial flexibil-ity on the value of shares of Brazilian companies during two different economic moments of the pandemic, namely: the collapse and the stimulus to the economy."
  },
  {
    "objectID": "publications/hedge.html",
    "href": "publications/hedge.html",
    "title": "Hedge dinâmicos: Uma evidência para os contratos futuros brasileiros",
    "section": "",
    "text": "Abstract\nThe farming sector is changing, growing and occupying a position of prominence in the economy. These transformations start to demand a bigger concern with the management of risks of the activity. In this direction, the contracts traded at BM&F had become efficient instruments in the reduction of the market risk, through an operation called hedge. However, there is still the necessity of improving of the econometrical techniques for the estimation of the optimal hedge ratio, therefore, it is observed in the brazilian literature that the majority of the works doesn’t consider some aspects of the behavior of the series of returns. Thus, the present work seeks to analyze two methods for the calculation of these hedge optimal ratio, the conventional model of regression and the bivariate GARCH BEKK model that considers the conditional correlations of the series. The preliminary analysis of the results indicates that the hedge optimal ratio is not constant through time, suggesting that the use of models that consider the time dependence of the series is more realistic. The methodology is applied to the prices of the beef cattle commodity."
  },
  {
    "objectID": "publications/overconfidence.html",
    "href": "publications/overconfidence.html",
    "title": "Overconfidence in Managerial Decision-Making among Brazilian Accountants and Managers: An Experimental Study",
    "section": "",
    "text": "Abstract\nThe aim of this work is to analyze, by means of an experiment, if the type (positive or negative) and the level (simple or complex) of financial information influence of overconfidence of entrepreneurs and accountants in a managerial decision-making process. The design consists of a 2 x 2 factorial experiment, with four treatments, with the type and level of accounting information as experimental factors. The research was applied to a sample of 68 managers, 86 accountants, and 118 people with different activities (control group). The results showed that the majority of participants present the Overconfidence bias, in the first test, without differentiation of information. They also presented significant evidence that overconfidence can be influenced by the type of information, but not by its level. Moreover, the analysis suggested that the profile of the participants influences the confidence in the projections conducted. The research has shown that the type of financial information influences the overconfidence of entrepreneurs and accountants."
  },
  {
    "objectID": "publications/system_dynamics.html",
    "href": "publications/system_dynamics.html",
    "title": "A System Dynamics Model for Sales and Operations Planning: An Integrated Analysis for the Lime Industry",
    "section": "",
    "text": "Abstract\nThe use of system dynamics techniques to model the sales and operations planning (S&OP), associated with the economic and financial processes, is an innovative proposal. The objectives of this article are to model and simulate the S&OP process integrated with the financial management in a Brazilian lime processing industry, based on the system dynamics approach. Initially, the model was validated. Then, over twenty scenarios were simulated to assess the behavior of the system with its key factors variation. In the microenvironment scenarios, the company’s internal perspective was the only element taken into account. In turn, regarding the macro environment scenarios, the basis was the projection of lime consumption related to the country’s GDP. The results have genuinely contributed to the industry researched, since the lime processing industry is struggling with obtaining enough supply due to lime acquisition price fluctuations and, consequently, the oscillation of its production costs."
  },
  {
    "objectID": "publications/toxic_metals.html",
    "href": "publications/toxic_metals.html",
    "title": "Deposition of Potentially Toxic Metals in the Soil from Surrounding Cement Plants in a Karst Area of Southeastern Brazil",
    "section": "",
    "text": "Abstract\nCement factories are the main sources of environmental pollutants among the different industrial activities, including soil contamination by potentially toxic metals. The karst region of Southeastern Brazil is known for the implementation of large cement producing facilities. This study aims to evaluate whether there is an increase in the concentration of PTM in the soil surrounding the cement plants and to estimate their harmfulness to both local human population and environment. In total, 18 soil samples were collected from the surroundings of three cement plants as well as four soil samples from areas outside the influence of cement plants and concentration of the following potentially toxic metals (PTM) were estimated: Cd, Pb, Co, Cu, Cr, Mn, Ni, and Zn. The results revealed that all PTM concentrations from cement plant surroundings were significantly higher than PTM concentrations from control areas and no PTM concentrations from CPS or CA soil samples exceeded national and global contamination thresholds. However, Igeo Index indicated low level soil contamination by Pb, Cu, and Cr, as well as high levels for Co. We could not verify significant non-carcinogenic risk to health for any soil sample, but carcinogenic risk analysis revealed different levels of carcinogenic risk among the sampled locations, for both adults and children. Our results indicate that exclusively evaluating the concentration of potentially toxic metals is not enough to verify the potential harmful effects of cement production for the surrounding population. Here we evidence that additional indices, based on both contamination indices and health risk assessments, should be considered for better evaluation of the impacts of cement production activity."
  }
]